{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages needed for this part\n",
    "# create spark and sparkcontext objects\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml import feature, regression, Pipeline, evaluation\n",
    "\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark.sql.functions import isnan, when, count\n",
    "from pyspark import sql\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3114617f16c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Importing csv files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mitems_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"items.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstores_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stores.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataFrameReader\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \"\"\"\n\u001b[0;32m--> 791\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrameReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Importing csv files\n",
    "test_df = spark.read.csv(\"test.csv\", header=True).limit(120000)\n",
    "train_df = spark.read.csv(\"train.csv\", header=True)\n",
    "items_df = spark.read.csv(\"items.csv\", header=True)\n",
    "stores_df = spark.read.csv(\"stores.csv\", header=True)\n",
    "holidays_df = spark.read.csv(\"holidays_events.csv\", header=True)\n",
    "transactions_df = spark.read.csv(\"transactions.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming some columns and dropping some unnecessary columns\n",
    "stores_df = stores_df.withColumnRenamed(\"type\",\"store_type\")\n",
    "holidays_df = holidays_df.withColumnRenamed(\"type\",\"holiday_type\")\n",
    "holidays_df = holidays_df.drop('description','transferred')\n",
    "train_df = train_df.drop('id')\n",
    "test_df = test_df.drop('id')\n",
    "# stores_df = stores_df.drop(\"city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coverting required columns to integer\n",
    "train_df = train_df.withColumn(\"store_nbr\", train_df[\"store_nbr\"].cast(IntegerType()))\n",
    "train_df = train_df.withColumn(\"item_nbr\", train_df[\"item_nbr\"].cast(IntegerType()))\n",
    "train_df = train_df.withColumn(\"unit_sales\", train_df[\"unit_sales\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"store_nbr\", test_df[\"store_nbr\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"item_nbr\", test_df[\"item_nbr\"].cast(IntegerType()))\n",
    "items_df = items_df.withColumn(\"item_nbr\",items_df[\"item_nbr\"].cast(IntegerType()))\n",
    "items_df = items_df.withColumn(\"class\",items_df[\"class\"].cast(IntegerType()))\n",
    "items_df = items_df.withColumn(\"perishable\",items_df[\"perishable\"].cast(IntegerType()))\n",
    "stores_df = stores_df.withColumn(\"store_nbr\",stores_df[\"store_nbr\"].cast(IntegerType()))\n",
    "stores_df = stores_df.withColumn(\"cluster\",stores_df[\"cluster\"].cast(IntegerType()))\n",
    "transactions_df = transactions_df.withColumn(\"store_nbr\",transactions_df[\"store_nbr\"].cast(IntegerType()))\n",
    "transactions_df = transactions_df.withColumn(\"transactions\",transactions_df[\"transactions\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(False, 0.001,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.sample(False, 0.1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining the dataframe into train_df\n",
    "train_df = train_df.join(stores_df, on = 'store_nbr', how = 'left')\n",
    "train_df = train_df.join(items_df, on = 'item_nbr', how = 'left')\n",
    "train_df = train_df.join(holidays_df, on = 'date', how = 'left')\n",
    "train_df = train_df.join(transactions_df, on = ['store_nbr','date'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining the dataframes into test_df\n",
    "test_df = test_df.join(stores_df, on = 'store_nbr', how = 'left')\n",
    "test_df = test_df.join(items_df, on = 'item_nbr', how = 'left')\n",
    "test_df = test_df.join(holidays_df, on = 'date', how = 'left')\n",
    "test_df = test_df.join(transactions_df, on = ['store_nbr','date'], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling null values with some value\n",
    "train_df = train_df.fillna({'onpromotion':'False','holiday_type':'No Holiday','locale':'Not Applicable',\\\n",
    "                            'locale_name':'Not Applicable', 'transactions':1884})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.fillna({'onpromotion':'False','holiday_type':'No Holiday','locale':'Not Applicable',\\\n",
    "                            'locale_name':'Not Applicable', 'transactions':1884})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = (train_df.filter(train_df.unit_sales > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = (train_df.filter(train_df.unit_sales > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = pyspark.sql.functions.split(train_df['date'], '-')     \n",
    "train_df= train_df.withColumn('Year', split_date.getItem(0))\n",
    "train_df= train_df.withColumn('Month', split_date.getItem(1))\n",
    "train_df= train_df.withColumn('Day', split_date.getItem(2))\n",
    "train_df = train_df.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = pyspark.sql.functions.split(test_df['date'], '-')     \n",
    "test_df= test_df.withColumn('Year', split_date.getItem(0))\n",
    "test_df= test_df.withColumn('Month', split_date.getItem(1))\n",
    "test_df= test_df.withColumn('Day', split_date.getItem(2))\n",
    "test_df = test_df.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.withColumn(\"Day\", test_df[\"Day\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"Month\", test_df[\"Month\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"Year\", test_df[\"Year\"].cast(IntegerType()))\n",
    "train_df = train_df.withColumn(\"Day\", train_df[\"Day\"].cast(IntegerType()))\n",
    "train_df = train_df.withColumn(\"Month\", train_df[\"Month\"].cast(IntegerType()))\n",
    "train_df = train_df.withColumn(\"Year\", train_df[\"Year\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA with Onehotencoded Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1 = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputcolumns = [\"onpromotion\",\"state\",\"store_type\",\"family\",\"city\",\"holiday_type\",\"locale\",\"locale_name\"]\n",
    "indexer = [StringIndexer(stringOrderType = 'alphabetAsc', inputCol = col, outputCol = \"{0}_index\".format(col)) for col in inputcolumns]\n",
    "encoder = [OneHotEncoder(inputCol = idx.getOutputCol(), outputCol = \"{0}_feat\".format(idx.getOutputCol()),dropLast = False) for idx in indexer]\n",
    "assembler = VectorAssembler(inputCols=['onpromotion_index_feat','state_index_feat','store_type_index_feat'\\\n",
    "                                      ,'family_index_feat','city_index_feat','holiday_type_index_feat','locale_index_feat','locale_name_index_feat'\\\n",
    "                                      ,'store_nbr', 'item_nbr','cluster','class','perishable','Month','Day','transactions']\\\n",
    "                            ,outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=indexer + encoder + [assembler])\n",
    "model = pipeline.fit(train_df1)\n",
    "transformed = model.transform(train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputcolumns = [\"onpromotion\",\"state\",\"store_type\",\"family\",\"city\",\"holiday_type\",\"locale\",\"locale_name\"]\n",
    "testindexer = [StringIndexer(stringOrderType = 'alphabetAsc', inputCol = col, outputCol = \"{0}_index\".format(col)) for col in inputcolumns]\n",
    "testencoder = [OneHotEncoder(inputCol = idx.getOutputCol(), outputCol = \"{0}_feat\".format(idx.getOutputCol()),dropLast = False) for idx in indexer]\n",
    "testassembler = VectorAssembler(inputCols=['onpromotion_index_feat','state_index_feat','store_type_index_feat'\\\n",
    "                                      ,'family_index_feat','city_index_feat','holiday_type_index_feat','locale_index_feat','locale_name_index_feat'\\\n",
    "                                      ,'store_nbr', 'item_nbr','cluster','class','perishable','Month','Day','transactions']\\\n",
    "                            ,outputCol=\"features\")\n",
    "testpipeline = Pipeline(stages=indexer + encoder + [assembler])\n",
    "testmodel = pipeline.fit(test_df)\n",
    "testtransformed = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the results\n",
    "# transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = transformed.drop('onpromotion','onpromotion_index','state','state_index','city','city_index',\\\n",
    "                               'store_type','store_type_index','family','family_index','holiday_type','holiday_type_index',\\\n",
    "                               'locale','locale_index','locale_name','locale_name_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtransformed = testtransformed.drop('onpromotion','onpromotion_index','state','state_index','city','city_index',\\\n",
    "                               'store_type','store_type_index','family','family_index','holiday_type','holiday_type_index',\\\n",
    "                               'locale','locale_index','locale_name','locale_name_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df1 = transformed.drop('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df, validation_df = transformed.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcacols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assembler2 = feature.VectorAssembler(inputCols = ['store_nbr','item_nbr','cluster','class','perishable'\\\n",
    "# #                                                  ,'transactions','Month','Day','features'],outputCol='pcafeatures')\n",
    "# std_scaled2 = feature.StandardScaler(inputCol='features', outputCol='standardizedFeatures')\n",
    "# pca3 = feature.PCA(k=17, inputCol='standardizedFeatures', outputCol='pc')\n",
    "# dpipe_pca = Pipeline(stages = [std_scaled2, pca3]).fit(training_df)\n",
    "# dtrain = dpipe_pca.transform(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_model = dpipe_pca.stages[-1]\n",
    "# pc1 = np.absolute(pca_model.pc.toArray()[:, 0]).tolist()\n",
    "# pc2 =np.absolute(pca_model.pc.toArray()[:, 1]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainedVariance = dpipe_pca.stages[-1].explainedVariance\n",
    "# explainedVariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# y = explainedVariance.toArray().tolist()\n",
    "# y = np.sort(np.cumsum(y))/np.sum(y)\n",
    "# plt.plot(y)\n",
    "# plt.xlabel('number of components')\n",
    "# plt.ylabel('Explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineartrainingdf, linearvalidationdf = transformed.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcacols = [x for x in lineartrainingdf.columns]\n",
    "pcacols.pop(2)\n",
    "pcacols.pop(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcols = pcacols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.select(fn.max(\"unit_sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.select(fn.min(\"unit_sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodel1 = Pipeline(stages=[\n",
    "#     feature.VectorAssembler(inputCols = pcacols, outputCol = 'features'),\n",
    "    regression.LinearRegression(featuresCol='features', labelCol='unit_sales')  \n",
    "]).fit(lineartrainingdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lrmodel1.transform(linearvalidationdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = fn.sqrt(fn.mean((fn.col('unit_sales') - fn.col('prediction'))**2)).alias('rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodel1.transform(linearvalidationdf).select(rmse).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = lrmodel1.stages[-1].coefficients.toArray()\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_col_list = ['item_nbr','store_nbr','cluster','class','perishable','Month','Day']\n",
    "\n",
    "l1 = model.stages[0].labels\n",
    "for i in l1:\n",
    "    label1 = i.replace(' ','_')\n",
    "    sales_col_list.append('onpromotion_'+ label1)\n",
    "    \n",
    "l2 = model.stages[1].labels\n",
    "for i in l2:\n",
    "    label2 = i.replace(' ','_')\n",
    "    sales_col_list.append('state_'+ label2)\n",
    "    \n",
    "l3 = model.stages[2].labels\n",
    "for i in l3:\n",
    "    label3 = i.replace(' ','_')\n",
    "    sales_col_list.append('store_type_'+ label3)\n",
    "    \n",
    "l4 = model.stages[3].labels\n",
    "for i in l4:\n",
    "    label4 = i.replace(' ','_')\n",
    "    sales_col_list.append('family_'+ label4)\n",
    "    \n",
    "l5 = model.stages[4].labels\n",
    "for i in l5:\n",
    "    label5 = i.replace(' ','_')\n",
    "    sales_col_list.append('holiday_type_'+ label5)\n",
    "    \n",
    "l6 = model.stages[5].labels\n",
    "for i in l6:\n",
    "    label6= i.replace(' ','_')\n",
    "    sales_col_list.append('locale_'+ label6)\n",
    "    \n",
    "l7 = model.stages[6].labels\n",
    "for i in l7:\n",
    "    label7= i.replace(' ','_')\n",
    "    sales_col_list.append('locale_name_'+ label7)\n",
    "    \n",
    "l8 = model.stages[7].labels\n",
    "for i in l8:\n",
    "    label8 = i.replace(' ','_')\n",
    "    sales_col_list.append('city_'+ label8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(list(zip(sales_col_list, coef)),\\\n",
    "                                 columns = ['feature', 'Coefficient']).sort_values('Coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = feature_importance.loc[feature_importance['Coefficient']>0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df\n",
    "transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rftraining_df, rfvalidation_df = transformed.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pipe = Pipeline(stages = [regression.RandomForestRegressor(featuresCol='features', labelCol='unit_sales')]).fit(rftraining_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluation.RegressionEvaluator(labelCol='unit_sales', metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmseRF=evaluator.evaluate(RF_pipe.transform(rfvalidation_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmseRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcacols.pop(15)\n",
    "imp = RF_pipe.stages[-1].featureImportances.toArray()\n",
    "feature_importance = pd.DataFrame(list(zip(pcacols, imp)),\\\n",
    "                                 columns = ['feature', 'importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a.groupBy('Month').agg(fn.avg('unit_sales').alias('Unit_Sales'),fn.avg('prediction').alias('Predicted_Sales')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Month\",y=\"Unit_Sales\", data = x)\n",
    "sns.lineplot(x=\"Month\",y=\"Predicted_Sales\", data = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = a.groupBy('Month').agg(fn.sum('unit_sales').alias('Unit_Sales'),fn.sum('prediction').alias('Predicted_Sales')).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Month\",y=\"Unit_Sales\", data = x1)\n",
    "sns.lineplot(x=\"Month\",y=\"Predicted_Sales\", data = x1)\n",
    "plt.legend(labels=['Actual Sales','Predicted Sales'])\n",
    "plt.ylabel(\"Sum of Sales\")\n",
    "# plt.xlabel('Month')\n",
    "# plt.ylabel('Unit Sales')\n",
    "# plt.title('Unit Sales for each Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderate = transformed.select(\"item_nbr\",\"Day\",\"Month\",\"perishable\",\"class\",\"family_index_feat\",\"unit_sales\")\n",
    "moderate = moderate.groupby(\"item_nbr\",\"Month\",\"Day\",\"perishable\",\"class\",\"family_index_feat\").agg(fn.sum(\"unit_sales\"))\n",
    "lineartrainingdf2, linearvalidationdf2 = moderate.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineartrainingdf2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodcols = ['item_nbr','Month','Day','perishable','class','family_index_feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodel2 = Pipeline(stages=[\n",
    "    feature.VectorAssembler(inputCols = lrmodcols, outputCol = 'features'),\n",
    "    regression.LinearRegression(featuresCol='features', labelCol='sum(unit_sales)')  \n",
    "]).fit(lineartrainingdf2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodel2.transform(linearvalidationdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse2 = fn.sqrt(fn.mean((fn.col('sum(unit_sales)') - fn.col('prediction'))**2)).alias('rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodel2.transform(linearvalidationdf2).select(rmse2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Important Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderatef = transformed.select(\"item_nbr\",\"Day\",\"Month\",\"perishable\",\"class\",\"family_index_feat\",\"unit_sales\")\n",
    "moderatef = moderatef.groupby(\"item_nbr\",\"Month\",\"Day\",\"perishable\",\"class\",\"family_index_feat\").agg(fn.sum(\"unit_sales\"))\n",
    "randomtrainingdf2, randomvalidationdf2 = moderatef.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_modpipe = Pipeline(stages = [feature.VectorAssembler(inputCols = lrmodcols, outputCol = 'features'),\\\n",
    "                                regression.RandomForestRegressor(featuresCol='features', labelCol='sum(unit_sales)')]).fit(randomtrainingdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluation.RegressionEvaluator(labelCol='sum(unit_sales)', metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmseRF=evaluator.evaluate(RF_modpipe.transform(randomvalidationdf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmseRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testmod = testtransformed.select(\"item_nbr\",\"Day\",\"Month\",\"perishable\",\"class\",\"family_index_feat\",\"unit_sales\")\n",
    "# testmod = testmod.groupby(\"item_nbr\",\"Month\",\"Day\",\"perishable\",\"class\",\"family_index_feat\").agg(fn.sum(\"unit_sales\"))\n",
    "# randomtrainingdf2, randomvalidationdf2 = moderatef.randomSplit([0.7, 0.3], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testmodel = Pipeline(stages=[\n",
    "#     feature.VectorAssembler(inputCols = pcacols, outputCol = 'features'),\n",
    "#     regression.RandomForestRegressor(featuresCol='features', labelCol='sum(unit_sales)')  \n",
    "# ]).fit(testtransformed)\n",
    "final_model = RF_modpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmoderatef = testtransformed.select(\"item_nbr\",\"Day\",\"Month\",\"perishable\",\"class\",\"family_index_feat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictSales(M, I):\n",
    "    abc = testmoderatef.filter(fn.col('Month') == M)\n",
    "    abc = abc.filter(fn.col('item_nbr') == I)\n",
    "    data = final_model.transform(abc)\n",
    "    sales = data.select(fn.sum('prediction')).alias(\"Predicted Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictSales(2,302952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.transform(randomvalidationdf2).groupby('Month','item_nbr').agg(fn.sum('sum(unit_sales)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
